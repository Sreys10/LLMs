{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846358d4",
   "metadata": {},
   "source": [
    "## Dense retrieval example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d20a7",
   "metadata": {},
   "source": [
    " Let’s take a look at a dense retrieval example by using Cohere to search the\n",
    " Wikipedia page for the film Interstellar. In this example, we will do the\n",
    " following:\n",
    " 1. Get the text we want to make searchable and apply some light\n",
    " processing to chunk it into sentences.\n",
    " 2. Embed the sentences.\n",
    " 3. Build the search index.\n",
    " 4. Search and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731e8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np \n",
    "import cohere\n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becb9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key= '0IAnuUYnGXylLb1C288TXaWLDJiENP32RqGbB9U8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694d4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and retrive a cohere api key from os.cohere.ai\n",
    "co= cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253afe08",
   "metadata": {},
   "source": [
    "Getting the text archive and chunking it\n",
    "\n",
    "Let’s use the first section of the Wikipedia article on the film Interstellar.\n",
    "We’ll get the text, then break it into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0676b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    " Interstellar is a 2014 epic science fiction film co-written, \n",
    "directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, \n",
    "Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is struggling to \n",
    "survive, the film follows a group of astronauts who travel \n",
    "through a wormhole near Saturn in search of a new home for \n",
    "mankind.\n",
    " Brothers Christopher and Jonathan Nolan wrote the screenplay, \n",
    "which had its origins in a script Jonathan developed in 2007. \n",
    "Caltech theoretical physicist and 2017 Nobel laureate in \n",
    "Physics[4] Kip Thorne was an executive producer, acted as a \n",
    "scientific consultant, and wrote a tie-in book, The Science of \n",
    "Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in \n",
    "the Panavision anamorphic format and IMAX 70 mm. \n",
    "Principal photography began in late 2013 and took place in \n",
    "Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects and \n",
    "the company Double Negative created additional digital effects.\n",
    " Interstellar premiered on October 26, 2014, in Los Angeles. \n",
    "In the United States, it was first released on film stock, \n",
    "expanding to venues using digital projectors. \n",
    "The film had a worldwide gross over $677 million (and $773 \n",
    "million with subsequent re-releases), making it the tenth-highest \n",
    "grossing film of 2014. \n",
    "It received acclaim for its performances, direction, screenplay, \n",
    "musical score, visual effects, ambition, themes, and emotional \n",
    "weight. \n",
    "It has also received praise from many astronomers for its \n",
    "scientific accuracy and portrayal of theoretical astrophysics. \n",
    "Since its premiere, Interstellar gained a cult following,[5] and \n",
    "now is regarded by many sci-fi experts as one of the best \n",
    "science-fiction films of all time.\n",
    " Interstellar was nominated for five awards at the 87th Academy \n",
    "Awards, winning Best Visual Effects, and received numerous other \n",
    "accolades\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db95f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into list of sentences\n",
    "texts= text.split('.')\n",
    "\n",
    "#clean up to remove emplty spaces\n",
    "texts= [t.strip('\\n') for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc451d5b",
   "metadata": {},
   "source": [
    "Embedding the text chunks\n",
    "\n",
    "\n",
    " Let’s now embed the texts. We’ll send them to the Cohere API, and get back\n",
    " a vector for each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f661566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4096)\n"
     ]
    }
   ],
   "source": [
    "#get the embeddings\n",
    "response= co.embed(\n",
    "    texts= texts, \n",
    "    input_type= 'search_document',\n",
    "\n",
    ").embeddings\n",
    "\n",
    "embeds= np.array(response)\n",
    "print(embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b3129",
   "metadata": {},
   "source": [
    "Building the search index\n",
    "\n",
    " Before we can search, we need to build a search index. An index stores the\n",
    " embeddings and is optimized to quickly retrieve the nearest neighbors even\n",
    " if we have a very large number of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28142d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import faiss #used for fast similarity search and clustering of dense vectors\n",
    "dim=embeds.shape[1]\n",
    "index= faiss.IndexFlatL2(dim)\n",
    "print(index.is_trained)\n",
    "index.add(np.float32(embeds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bf50d",
   "metadata": {},
   "source": [
    "Search the index\n",
    "\n",
    " We can now search the dataset using any query we want. We simply embed\n",
    " the query and present its embedding to the index, which will retrieve the\n",
    " most similar sentence from the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for searching\n",
    "\n",
    "def search(query, number_of_results=5):\n",
    "\n",
    "    #get the query embeds\n",
    "    query_embed= co.embed(texts= [query], input_type='search_query').embeddings[0]\n",
    "\n",
    "    #retrieve the nearest neighbour\n",
    "    distnaces,similar_items_ids= index.search(np.float32([query_embed]), number_of_results)\n",
    "\n",
    "    #get the reseults\n",
    "    texts_np = np.array(texts)\n",
    "    results=pd.DataFrame(data= {'texts': texts_np[similar_items_ids[0]],\n",
    "                            'distances': distnaces[0]})\n",
    "    \n",
    "    #print nd return the results\n",
    "    print(f\"Query:'{query}' \\n Nearest Neighbours:\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ffac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:'how precise was the science' \n",
      " Nearest Neighbours:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIt has also received praise from many astro...</td>\n",
       "      <td>10267.427734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nSince its premiere, Interstellar gained a c...</td>\n",
       "      <td>12490.473633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nCaltech theoretical physicist and 2017 Nobe...</td>\n",
       "      <td>12507.566406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nInterstellar uses extensive practical and m...</td>\n",
       "      <td>12546.205078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nCinematographer Hoyte van Hoytema shot it o...</td>\n",
       "      <td>13720.408203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts     distances\n",
       "0   \\nIt has also received praise from many astro...  10267.427734\n",
       "1   \\nSince its premiere, Interstellar gained a c...  12490.473633\n",
       "2   \\nCaltech theoretical physicist and 2017 Nobe...  12507.566406\n",
       "3   \\nInterstellar uses extensive practical and m...  12546.205078\n",
       "4   \\nCinematographer Hoyte van Hoytema shot it o...  13720.408203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "query =\"how precise was the science\"\n",
    "results= search(query)\n",
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a91eb",
   "metadata": {},
   "source": [
    "Reranking example\n",
    "\n",
    " A reranker takes in the search query and a number of search results, and\n",
    " returns the optimal ordering of these documents so the most relevant ones\n",
    " to the query are higher in ranking. Cohere’s Rerank endpoint is a simple\n",
    " way to start using a first reranker. We simply pass it the query and texts and\n",
    " get the results back. We don’t need to train or tune it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d5a4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.15239799  \n",
      "It has also received praise from many astronomers for its \n",
      "scientific accuracy and portrayal of theoretical astrophysics\n",
      "1 0.050354082  \n",
      "The film had a worldwide gross over $677 million (and $773 \n",
      "million with subsequent re-releases), making it the tenth-highest \n",
      "grossing film of 2014\n",
      "2 0.0350424  Interstellar is a 2014 epic science fiction film co-written, \n",
      "directed, and produced by Christopher Nolan\n"
     ]
    }
   ],
   "source": [
    "query =\"how precise was the science\"\n",
    "results= co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
    "results.results\n",
    "\n",
    "for idx, result in enumerate(results.results):\n",
    "    print(idx, result.relevance_score, result.document.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79feed19",
   "metadata": {},
   "source": [
    "# Grounded Generation with an LLM API\n",
    "\n",
    " Let’s now turn our search system into a RAG system. We do that by adding\n",
    " an LLM to the end of the search pipeline. We present the question and the\n",
    " top retrieved documents to the LLM, and ask it to answer the question\n",
    "given the context provided by the search results\n",
    "\n",
    "This generation step is called grounded generation because the retrieved\n",
    " relevant information we provide the LLM establishes a certain context that\n",
    " grounds the LLM in the domain we’re interested in\n",
    "\n",
    "Let’s look at how to add a grounded generation step after the search results\n",
    " to create our first RAG system. For this example, we’ll use Cohere’s\n",
    " managed LLM, which builds on the search systems we’ve seen earlier in\n",
    " the chapter. We’ll use embedding search to retrieve the top documents, then\n",
    " we’ll pass those to the co.chat endpoint along with the questions to\n",
    " provide a grounded answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4828a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:'income generated' \n",
      " Nearest Neighbours:\n",
      "The film Interstellar had a worldwide gross of over $677 million, and $773 million with subsequent re-releases.\n"
     ]
    }
   ],
   "source": [
    "query= \"income generated\"\n",
    "\n",
    "#1 - Retrival\n",
    "#we will use the embedding search\n",
    "results= search(query)\n",
    "\n",
    "#Grounded generation\n",
    "\n",
    "docs_dict= [{'text': text} for text in results['texts']]\n",
    "response= co.chat(\n",
    "    message = query, \n",
    "    documents= docs_dict\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ea16f",
   "metadata": {},
   "source": [
    "## RAG with Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f49ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\utils\\utils.py:157: UserWarning: WARNING! repo_id is not default parameter.\n",
      "                repo_id was transferred to model_kwargs.\n",
      "                Please confirm that repo_id is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\utils\\utils.py:157: UserWarning: WARNING! n_gpu_layer is not default parameter.\n",
      "                n_gpu_layer was transferred to model_kwargs.\n",
      "                Please confirm that n_gpu_layer is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\llms\\llamacpp.py:143\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp \u001b[38;5;66;03m#microsoft/Phi-3-mini-4k-instruct-gguf\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m llm\u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-mini-4k-instruct-gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhi-3-mini-4k-instruct-fp16.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\load\\serializable.py:90\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\llms\\llamacpp.py:145\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     )\n\u001b[0;32m    151\u001b[0m model_path \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    152\u001b[0m model_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    169\u001b[0m ]\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "from langchain import LlamaCpp #microsoft/Phi-3-mini-4k-instruct-gguf\n",
    "\n",
    "\n",
    "llm= LlamaCpp(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "\tmodel_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layer=-1,\n",
    "    max_tokens = 500,\n",
    "    n_ctx = 2048,\n",
    "    seed = 42,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaddea4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model= HuggingFaceEmbeddings(\n",
    "    model_name='thenlper/gte-small'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a13386e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 3\u001b[0m db\u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(\u001b[43mtexts\u001b[49m, embedding_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db= FAISS.from_texts(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a3ebe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Rag Pipeline\u001b[39;00m\n\u001b[0;32m     21\u001b[0m rag\u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m---> 22\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m, \n\u001b[0;32m     23\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m     retriever\u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[0;32m     25\u001b[0m     chain_type_kwargs \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     }\n\u001b[0;32m     29\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "#create a prompt template\n",
    "template = \"\"\"<|user|>\n",
    " Relevant information:\n",
    " {context}\n",
    " Provide a concise answer the following question using the \n",
    "relevant information provided above:\n",
    " {question}<|end|>\n",
    " <|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= template,\n",
    "    input_variables = ['context', 'question']\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#Rag Pipeline\n",
    "\n",
    "rag= RetrievalQA.from_chain_type(\n",
    "    llm = llm, \n",
    "    chain_type= 'stuff',\n",
    "    retriever= db.as_retriever(),\n",
    "    chain_type_kwargs ={\n",
    "        'prompt': prompt,\n",
    "        'verbose': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f5628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
