{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500b7a31",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228091cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#load MNLI Dataset from Glue \n",
    "# 0- entailment, 1- contradiction, 2- neutral\n",
    "\n",
    "train_dataset= load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50000))\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e5e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': \"and it's it's quite a bit i think six something is the state and and uh the rest of the pie goes elsewhere but we're in a particular part of the state that's pretty well off so it's it's like we get a lot of that back as far as local taxation goes\",\n",
       " 'hypothesis': 'I do not know exactly where the local taxes go.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4252206",
   "metadata": {},
   "source": [
    "### train model\n",
    "\n",
    "Now that we have our dataset with training examples, we will need to\n",
    " create our embedding model. We typically choose an existing sentence\n",
    "transformers model and fine-tune that model, but in this example, we\n",
    " are going to train an embedding from scratch.\n",
    " This means that we will have to define two things. First, a pretrained\n",
    " Transformer model that serves as embedding individual words. We will use\n",
    " the BERT base model (uncased) as it is a great introduction model.\n",
    " However, many others exist that also have been evaluated using\n",
    " sentence-transformers. Most notably, microsoft/mpnet\n",
    "base often gives good results when used as a word embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b04e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e8f53aa53a445b91c0b3a33e67be58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02e553ad5e14be28d28a9ae5fd0b48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108c30fd98324bd881ed98493a4d5744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba75160de1eb40b9b8b09ad41ffacc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a968a851249a3a5dc17ada0a35c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHREYAS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#use a base model\n",
    "embedding_model = SentenceTransformer('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aeb1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
